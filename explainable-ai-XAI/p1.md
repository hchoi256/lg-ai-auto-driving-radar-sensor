
****
### Terms
- Supervised learning
- Explainable AI (XAI)
  - Explainability / Interpretability
  - Taxonomy of XAI Methods
    - Local vs. Global
    - White box vs. Black box
    - Intrinsic vs. Post hoc
    - Model specific vs. Model agnostic
  - Simple Gradient method
  - SmoothGrad

# Supervised (Deep) Learning
![image](https://user-images.githubusercontent.com/39285147/179156118-6b857604-efbd-411c-afa1-0b3559a4329c.png)

Supervised (deep) learning has made a huge progress!
- Image classification, speech recognition, machine translation, etc.

## Limitation of Supervised Learning: Deep learning models are **EXTREMELY** complex
대용량 학습 데이터로 부터 학습하는 모델 구조가 점점 더 복잡해지고 이해하기 어려워짐.

### 1. End to end learning becomes a black box!
![image](https://user-images.githubusercontent.com/39285147/179186432-74ff479c-0603-4c12-95d6-0bf6f41187d5.png)

> End-to-end
>> 딥러닝은 '종단간 기계학습(end-to-end deep learning)' 이라고도 불린다. 여기서 '종단간' 은 처음부터 끝까지라는 의미로, 입력에서 출력까지 '파이프라인 네트워크' 없이 한 번에 처리한다는 뜻이다.

### 2. Problem happens when models applied to make critical decisions
![image](https://user-images.githubusercontent.com/39285147/179156427-a4ecf10d-3ef4-4175-87d3-e1e4fe51a54b.png)

e.g , self driving cars, medical diagnosis, loan approval, AI interview, etc.

### 3. Deep learning models can be biased!

## 해결점
We need to **Explainable AI (XAI)** to detect the bias!

# Explainable AI (XAI)
모델이 왜 이렇게 인종과 관련하여 잘못된 예측 결과를 내었고, 이러한 평향성 바이어스를 고치기 위해서 어떻게 해야 할지 등을 알아내기 위해 모델의 결과를 설명할 수 있는 설명 가능한 인공지능이다.

예측 시스템을 간단하게 설명해봄으로써 도메인에서 오류/문제를 찾아내는 방식으로 동작한다.

## Reliability & Robustness
[*Pascal VOC 2007 classification*]
![image](https://user-images.githubusercontent.com/39285147/179186982-41db7a02-a5ea-435f-a0a6-ae2b536f53ed.png)

*Pascal*: 이미지 분류 연구로 쓰인 기법으로, 이미지의 어느 부분을 보고 class를 판별하였는지 빨간색으로 표시한다.

**XAI 기법**은 Pascal 실험이 이미지에 포함된 워터마크로 class를 분류하는 오류를 포함하는 모델/데이터셋의 오류를 색출한다.

## Fairness COMPAS crime prediction (편향성)
[*COMPAS*]
![image](https://user-images.githubusercontent.com/39285147/179187567-70d4b6fa-4c3f-4913-8850-559ba7637968.png)

*COMPAS*: 어떤 범죄자를 풀어줬을 때, 그 사람이 다시 범죄를 저질러서 감옥으로 돌아올 확률를 도출해낸다.

*XAI 기법*은 COMPAS가 False Positive(흑인이 범죄를 저지르지 않았는데도 나중에 재범할 것이라고 예측) 혹은 False Negative(백인이 범죄를 저질렀는데도 나중에 범죄를 저지르지 않을 것이다)라는 잘못된 인식 오류가 발생하는 것을 발견한다.

## Critical systems
- Self-driving car (자동차 사고 원인 판단하는 것이 중요하다)
- COVID19 classification (왜 알고리즘이 그런 예측 결과를 냈는지 설명하는 것은 신뢰 여부를 결정한다)

# Explainability / Interpretability?
### Interpretability
– Interpretability is the degree to which a human can understand the cause of a decision 
– Interpretability is the degree to which a human can consistently predict the model's results
– An explanation is the answer to why question
– Or in dictionary

### Explainability
- 사람이 모델을 쓸 때 그 동작을 이해하고 신뢰할 수 있게 해주는 기계 학습 기술

# Taxonomy of XAI Methods
### Local vs. Global
- **Local**: Describes an individual prediction (주어진 특정 데이터에 대한 예측 결과를 개별적으로 설명)
- **Global**: Describes entire model behavior (전체 데이터셋에서 모델의 전반적인 행동을 설명)

### White box vs. Black box
- **White box**: Explainer can access the inside of model (모델의 내부 구조를 정확하게 알고 있는 상황에서 설명을 시도)
- **Black box**: Explainer can access only the output (모델의 내부 구조는 모른채, 단순히 모델의 입력과 출력만 가지고 설명을 시도)

## Intrinsic vs. Post hoc
- **Intrinsic**: Restricts the model complexity before training (모델의 복잡도 훈련 이전부터, 설명에 용이한 제안을 한 뒤 학습을 시킨 모델을 가지고 설명)
- **Post hoc**: Applies after the ML model is trained (임의 모델 훈련 이후 이 방법을 적용해서 그 모델의 행동을 설명)

### Model specific vs. Model agnostic
- **Model specific**: Some methods restricted to specific model classes (e.g., CAM requires global average pooling) (특정 모델 구조에만 적용 가능)
- **Model agnostic**: Some methods can be used for any model (모델 구조와 관하게 어느 모델에도 항시 적용)

## Linear model, Simple Decision Tree
![image](https://user-images.githubusercontent.com/39285147/179189844-ad8ff3bb-257d-4bf6-92cc-7d8fe19f0ffe.png)

**Global, White box, Intrinsic, Model specific**

## Grad CAM
![image](https://user-images.githubusercontent.com/39285147/179190062-608a96d2-edbb-4667-a870-48a5ebff6e30.png)

**Local, White box, Post hoc, Model agnostic*


## Outline
![image](https://user-images.githubusercontent.com/39285147/179199879-6ecfd9dc-78ab-4f79-9f5f-e2f50d52091b.png)

**Saliency map-based**
- 하나의 이미지 샘플이 모델의 입력으로 들어가면 그 샘플에 대한 예측 결과에 대한 설명을 이미지에서 중요한 부분을 하이라이트해서 보여준다. 

### Simple Gradient method
![image](https://user-images.githubusercontent.com/39285147/179200145-04b347ae-c680-4eb7-9347-5e6b33c1e343.png)

입력에 대한 모델의 Gradient로 설명을 제공한다 (딥러닝 모델 Back-Propagation으로 간단히 구현 가능하다).
- Gradient ↑, 해당 픽셀에 중요도 ↑

#### Examples
![image](https://user-images.githubusercontent.com/39285147/179200476-77a493ae-e38c-41c2-8e6b-ac369eb60a6d.png)

The gradient maps are visualized for the highest scoring class (top 1 class prediction)

### Strength / Weakness
Strength
- Easy to compute (via back propagation)

Weakness
- Becomes noisy (due to shattering gradient (조금씩 변화가 있는 같은 예측 결과값을 도출해내는 각 이미지들에 대한 설명은 다르게 나타날 수 있다)
  - 해결: **SmoothGrad**

### 해결: SmoothGrad
![image](https://user-images.githubusercontent.com/39285147/179200889-b6ce5429-4e1f-4cce-83a7-971a24a28850.png)

Noisy한 Gradient들을 많이 제거하고 평균적으로 남는 Gradient가 더 깨끗한 설명이 가능하다.

A simple method to address the **noisy** gradients
- Add some noise to the input and average!
- Averaging gradients of slightly perturbed input would **smoothen** the interpretation
- Typical heuristics
  - Expectation is approximated with Monte Carlo (around 50 runs)
  - 𝜎is set to be 10~20% of 𝑥𝑚𝑎𝑥−𝑥𝑚𝑖𝑛

#### Strength / Weakness
![image](https://user-images.githubusercontent.com/39285147/179201251-8c93b59a-0a4b-48e4-afd2-9ceefebccf16.png)

SmoothGrad seems to work better for *uniform* background

**Strength**
- Clearer interpretation via simple averaging
- Applicable to most sensitive maps

**Weakness**
- Computationally expensive! (Back propagation만큼 계산 수행해야한다)
