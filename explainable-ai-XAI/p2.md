****
### Terms
- CAM
  - GAP
- Grad-CAM
- Perturbation-based
  - LIME
  - RISE
- Influence function-based
  - Inception (CNN)

# Class Activation Map (CAM)
[*Constraint of CAM*]

![image](https://user-images.githubusercontent.com/39285147/179204300-922e632a-a70e-41aa-8099-b2f5d9dc1c1a.png)
![image](https://user-images.githubusercontent.com/39285147/179204393-4dd5c474-3ee9-4796-8091-6f69c98ae99f.png)

> 수식 해석
>> GAP: 픽셀 i,j에 대한 Activation들의 총합을 Activation 크기로 나눈 것
>>
>> Yc: 클래스 c에 대한 output 분류 score

![image](https://user-images.githubusercontent.com/39285147/179205670-716e1940-0623-476b-9bf6-c93df48584c2.png)

CAM이란 Saliency map 기반 설명 가능 방법으로, CNN이 입력으로 들어온 이미지를 분류할 때 "어떤 부분을 보고" 예측을 했는지를 알려주는 역할이다.
- CAM can localize objects in image.
- Segment the regions that have the value above 20% of the max value of the CAM and take the bounding box of it

Global average pooling (GAP) 이라는 특정 레이어를 만들고, 이를 활용하여 설명을 제공한다.
- GAP should be implemented before the softmax layer (이전 Activation 연산들에 선형결합으로 softmax layer을 취한다).

> GAP Layer
>> GAP(global average pooling)은 앞에서 설명한 Max(Average) Pooling 보다 더 급격하게 feature의 수를 줄인다. GAP의 목적은 feature를 1차원 벡터로 만들기 위함이다.
>> 
>> 각 Activation map의 모든 Activation들을 평균 내는 연산이다.
>> 
>>>> 각 Activation마다 하나의 숫자를 얻게되고, 그것들이 사진 속 동그라미들로써 표현된다. 그 숫자가 클수록 인풋과 연관성이 커서 결과 사진에서 보이는 바와같이 강조되어 표시된다.

### Applications
- Object detection
- Semantic segmentation

> Weakly supervised learning 
>> 각 이미지마다 class label만 주어져 있어도 그것을 활용하여 분류기를 학습한 후에 CAM 정보를 이용해서 더 복잡한 상기 방법을 해결하는 것

## Strength / Weakness
Strength
- It **clearly shows** what objects the model is looking at

Weakness
- **Model specific**: it can be applied only to models with limited architecture(GAP이 적용된 Layer)
- It can only be obtained at the last convolutional layer and this makes the interpretation **resolution coarse**
- 객체의 boundary까지 찾아내지는 못한다.

해결법
- **Grad-CAM*

# Grad-CAM
![image](https://user-images.githubusercontent.com/39285147/179210819-8d0a9573-7079-4fba-af90-13cdfedb0648.png)
![image](https://user-images.githubusercontent.com/39285147/179213811-12a8a5bb-839e-4e55-a98b-9eeb32d87d1a.png)

CAM을 Gradient 정보를 활용해서 확장한 설명 방법으로 GAP가 없어도 적용 가능하다.
- To calculate the channel-wise weighted sum, Grad-CAM substitute weights by average pooled gradient

## Strength / Weakness
Strength
- **Model agnostic**: can be applied to various output models

Weakness
- Average gradient sometimes is not accurate
  - Gradient ↑, 해당 Activation 출력 값 민감도 ↑ (절대적으로 옳은 중요도 X)

# CAM vs. Grad-CAM
CAM은 GAP가 있어야만 사용할 수 있다. 따라서 GAP가 없는 경우 FC layer를 GAP로 대체한 후 재학습을 시켜준 후 visualzation을 진행하였다. 

반면, Grad-CAM은 '범용적으로' CAM을 사용하기 위해 CAM의 가중치 w를 해당 Activation map의 Gradient에 기반한 GAP 값으로 대체한다.

# Perturbation-based
![image](https://user-images.githubusercontent.com/39285147/179214483-7aaba83f-21a5-4693-954a-313753410034.png)

모델의 정확한 구조나 계수는 모르는 상태에서 그 모델에 대한 입출력만 가지고 있는 경우 설명하는 방법.
- 입력 데이터를 조금씩 바꾸면서 그에 대한 출력을 보고, 그 변화에 기반해서 설명한다.

## Local Interpretable Model agnostic Explanations (LIME)
![image](https://user-images.githubusercontent.com/39285147/179215251-db1a6ba2-d9f9-43d9-88e7-8256672213ad.png)

> 빨간색 포인트 주변에서는 선형 모델로 근사하여 표현하겠다는 의미.

어떤 분류기가 DL 모델처럼 매우 복잡한 비선형적 특징을 가지고 있어도 주어진 데이터 포인트에 대하여 아주 Local하게는 다 선형적인 모델로 근사화가 가능하다.
- 입력 데이터를 조금씩 바꾸면서 그에 대한 출력을 보고, 이렇게 나온 입출력 Pair(*purturbed된 이미지, 출력확률*)들을 간단한 선형 모델로 근사하여 설명한다.

### Strength / Weakness
Strength
- Black-box 설명 방법

> Black-box
>> DL 모델뿐 아니라 *주어진 입력*과 *그에 대한 출력*만 얻을 수 있다면 어떤 모델에 대해서도 다 적용할 수 있는 설명 방법

Weakness
- Computationally expensive (forward propagation 多)
- Hard to apply to certain kind of models when the underlying model is still locally non linear
- 객체 분류가 아니라 이미지 전체 분류에는 취약함

![image](https://user-images.githubusercontent.com/39285147/179216828-a8b38c14-1a06-41c8-a7b5-f44f0a6b291e.png)

### Image Example
![image](https://user-images.githubusercontent.com/39285147/179215409-90e5a86c-b308-47b5-9e4a-7e11f3b74ad1.png)
![image](https://user-images.githubusercontent.com/39285147/179216135-f255d30c-0fa6-47e5-800f-0a250bcdaf16.png)

1. Use super pixels as interpretable components
2. Perturb the super pixels and obtain the local interpretation model near the given example

> Superpixel
>> Superpixel이란 유사성을 지닌 픽셀들을 일정 기준에 따라 묶어서 만든 하나의 커다란 픽셀을 말한다
>> 
>> The saliency of LIME is relied on super pixels, which may not capture correct regions

## Randomized Input Sampling for Explanation (RISE)
![image](https://user-images.githubusercontent.com/39285147/179217215-b13a3da6-1207-4816-8212-e41b4c7d25c9.png)
![image](https://user-images.githubusercontent.com/39285147/179217375-104f5666-86ce-47d7-8af7-7bc25d6c2ae5.png)

LIME과 비슷하게 여러 번 입력을 perturb해서 설명을 구하는 Black-box 설명 방법이다.

여러 개의 랜덤 마스킹이 되어 있는 입력에 대한 출력 스코어(= 확률)를 구하고, 이 마스크들을 가중치를 둬서 평균을 냈을 때 나오는 것이 출력값인 Map이다.

### Strength / Weakness
Strength
- Much clear saliency map

Weakness
- High computational complexity (LIME보다 더 많은 randomly generated masked images 필요로 한다)
- Noisy due to sampling (# masked images에 따라 설명이 달라진다)

# Influence function-based
![image](https://user-images.githubusercontent.com/39285147/179217959-4cdc9394-131c-4323-b7cd-4cd54765c574.png)
![image](https://user-images.githubusercontent.com/39285147/179218224-f03d1d3c-619c-4a9d-aadc-36f117d62f61.png)

테이스 이미지에 하이라이트 표시로 설명한 앞선 두 기법(Saliency map-based, Perturbation-based)과 다르게, 테스트 이미지 분류에 가장 큰 영향을 미친 Training image가 해당 분류에 대한 Black-box 설명 방법이다.

## Influence function
![image](https://user-images.githubusercontent.com/39285147/179218358-8dc6efc2-3629-4279-a302-fdb0267ed9d5.png)

Measure the effect of removing a training sample on the test loss value

해당 테스트 이미지 분류에 있어서 Training image 샘플 하나를 제외하고 학습했을 때 분류 스코어가 얼만큼 변할지 근사하는 함수이다. 가장 큰 영향력을 행사한 Training 샘플을 설명으로 제공한다.

## SVM vs. Inception
![image](https://user-images.githubusercontent.com/39285147/179218722-3f02a285-fd96-4e58-ab74-aeb51a4e6652.png)

SVM
- Test image와 색깔이 비슷한 Training image를 찾는다.

Inception(CNN)
- 실제 Test image(열대어)와 비슷한 Training image를 찾는다 (제대로된 특징들을 더 잘 뽑아내서 학습한다).
