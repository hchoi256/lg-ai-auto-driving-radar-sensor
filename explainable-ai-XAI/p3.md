****
### Terms
- Quantitative Metrics
  - Human based visual assessment
  - Human annotation
  - Pixel perturbation
  - ROAR
- Sanity checks / Robustness
  - Model randomization
  - Adversarial attack
  - Adversarial model manipulation

![image](https://user-images.githubusercontent.com/39285147/179220453-c3bb879e-bda2-48e1-a406-6d9288dd98e1.png)

설명 방법들끼리 비교하는 방법론에 대한 연구

# Metrics
## 1. Human based visual assessment
![image](https://user-images.githubusercontent.com/39285147/179220775-664cfbc8-cfca-40d7-9343-c532db7e4191.png)

'사람들이 직접' XAI 방법들이 만들어낸 설명을 보고 비교 평가하는 것이다.

### AMT (Amazon Mechanical Turk) test

모델이 내놓은 예측 결과에 대해 사람이 어떤 예측인지 맞추는 방식으로 평가한다.

#### Weakness
Obtaining human assessment is very **expensive**

## 2. Human annotation
![image](https://user-images.githubusercontent.com/39285147/179225555-edf650b0-5201-49aa-ac1f-67d57c025d47.png)

Some metrics employ human annotations (localization and semantic segmentation) as a ground truth, and compare them with interpretation
- Pointing game
- Weakly supervised semantic segmentation

### Pointing game
![image](https://user-images.githubusercontent.com/39285147/179227693-6ddfb9cf-6fc8-4749-96a9-fd32324dda74.png)

Bounding box를 활용해서 평가하는 방법으로, 테스트 이미지에서 중요하다 생각되는 부분이 사람이 지정한 bounding box 내부에 위치하면 좋은 지표이다.

### Weakly supervised semantic segmentation
![image](https://user-images.githubusercontent.com/39285147/179231343-12c2dd1a-0160-4768-a9c8-58f21071f935.png)

어떤 이미지에 대해서 Classification label 만 주어져 있을 때, 그것을 활용하여 픽셀별로 객체의 Label을 예측하는 Semantic segmentation을 수행하는 방법이다.
- Setting: Pixel level label is not given during training
- This metric measures the mean IoU between interpretation and semantic segmentation label
- mIoU of GradCAM is 49.6%

> Weakly Supervised인 이유?
>> 픽셀별 정담 label이 다 주어져 있지 않기 때문

> IoU(Intersection over Union)
>> 정답 Map과 이렇게 만들어낸 Segmentation map이 얼마나 겹치는지를 평가하는 Metric

#### Weakness
Hard to make the human annotations

Such localization and segmentation labels are not a ground truth of interpretation

## 3. Pixel perturbation
### Motivation
![image](https://user-images.githubusercontent.com/39285147/179231549-2017a42a-6477-4cba-adcc-e158461aeaaf.png)

If we remove an important area in image, the logit value for class would be decreased

### 3-1. AOPC (Area Over the MoRF Perturbation Curve)
![image](https://user-images.githubusercontent.com/39285147/179231896-b0e36edc-dcae-468e-85c1-84ca277fa16e.png)

AOPC measures the decreases of logits from the replacement of the input patch in MoRF (Most Relevant First) order

주어진 이미지에 대해 XAI가 설명을 제공하면, 그 제공한 설명의 중요도 순서대로 각 픽셀들을 정렬할 수 있을 것이고, 그 순서대로 픽셀을 **교란*하였을 때, 원래 예측한 분류 스코어 값이 얼마나 빨리 바뀌는지를 측정하는 것이다.

> 교란
>> 중요하다 생각하는 부분을 랜덤한 픽셀로 변환하는 작업

### 3-2. Insertion and Deletion
![image](https://user-images.githubusercontent.com/39285147/179232840-c488b6e8-54c3-4b6a-ae6e-312152936e3c.png)

In Deletion curve, x axis is the percentage of the removed pixels in the MoRF order, and y axis is the class probability of the model

In Insertion curve, x axis is the percentage of the recovered pixels in the MoRF order, starting from gray image

### Strength / Weakness
Strength
- 사람의 직접적인 평가나 Annotation을 활용하지 않으면서도 객관적인, 정량적인 평가 지표를 얻을 수 있다는 데 장점이 있다.

Weakness
주어진 입력의 데이터를 지우거나 추가하면서 모델의 출력값을 보는데, 이 과정이 ML의 주요 과정을 위반하기도 한다.
- 어떤 픽셀을 지우고 랜덤 픽셀로 대체했을 때, 해당 이미지는 모델을 학습시킨 이미지 분포와 달라져서 출력 스코어의 변화가 정확하지 않을 수 있다.
  - 가령 랜덤하게 지운 모양이 동그라미라서, 이 모양 때문에 풍선이라고 모델이 착각할 수도 있다.

### 해결점
ROAR

## 4. ROAR ( RemOve And Retrain)
XAI이 생성한 이미지에서 중요한 픽셀들을 지우고 나서, 다시 *그 지워진 데이터*를 활용해서 모델을 재학습한 뒤 정확도가 얼마나 떨어졌나 평가하는 방법이다.

### Strength / Weakness
Strength
- Insertion이나 AOPC에 비해 더 정확한 판단이 가능하다.

Weakness
- Retraining everytime is computationally expensive! (픽셀 지운 후 모델을 매번 재학습해야한다)

# Sanity checks(온전성 채크)
![image](https://user-images.githubusercontent.com/39285147/179234232-4a3bf403-7b57-4888-9c79-986b9f2b4193.png)

개발된 XAI 방법의 신뢰성에 관한 연구

## 1. Model randomization
![image](https://user-images.githubusercontent.com/39285147/179234504-1bc7fefa-a9b0-467c-a18e-7967d84f850b.png)

랜덤 모델에 대한 설명이 뭉개지는(= **Edge detector*) 방법들은 신뢰할 수 없다.

> Edge detector
>> 에지에 해당하는 화소(edge pixels)를 찾는 과정이다.

## 2. Adversarial attack
![image](https://user-images.githubusercontent.com/39285147/179235288-16e5db11-7e86-4639-8331-7406b0806c36.png)

의도적으로 적개심을 품고 **입력값을 조작하여** 설명을 조작할 수 있는 여지가 존재 여부를 판단한다.
- 분류기의 예측을 유지하면서 설명 방법을 완전히 이상하게 공격할 수 있는지

### How to Achieve Adversarial Attack?
![image](https://user-images.githubusercontent.com/39285147/179235702-ed9ea7e1-0ab5-46c0-952a-a3c3bf6f1e3e.png)

- 결정경계가 smooth하지 않고 삐죽삐죽하기 때문에 공격 포인트 多
  - 조금만 방향을 틀어도 Gradient의 방향이 완전 달라지기 때문이다.

<details markdown="1">
<summary>Softplus(접기/펼치기)</summary>

![image](https://user-images.githubusercontent.com/39285147/179236096-51f98b3e-0e3b-4e6b-88ac-f033b4733a1d.png)![image](https://user-images.githubusercontent.com/39285147/179236303-9dcf68e8-5c38-4517-bf5c-e68c282d2e12.png)

ReLU를 사용하면 독수리와 배에 대한 설명이 구분이 안 되서 신뢰성이 없으나, Softplus를 사용하면 구분이 더 정확하게 가능하다.

</details>

> Softplus
>> 상기 문제점을 경감시키고자 사용되는 Activation Function이다.

## 3. Adversarial model manipulation
![image](https://user-images.githubusercontent.com/39285147/179236621-6e0d00cc-8d66-4a98-8652-c20ab64d8bb0.png)

의도적으로 적개심을 품고 **모델을 변형시켜** 설명을 조작할 수 있는 여지가 존재 여부를 판단한다.
- 모델이 편향됐는지 아닌지 여부 판단

**편향성*이 존재하는 모델을 재학습시키지 않고, 모델의 계수들을 조금만 조작해서 모델의 정확도는 변함없으나, 편향성이 존재함에도 설명이 정확하게 된 것처럼 유도가 가능하다.

> 편향성
>> 모델이 학습할 때 어떤 피처(i.e., sex)에 편향된 가중치를 두어 예측을 수행하는 것
