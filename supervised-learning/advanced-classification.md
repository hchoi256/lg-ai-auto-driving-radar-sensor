****
### Terms
- Advanced Classification
  - SVM
  - NN

# ANN
![image](https://user-images.githubusercontent.com/39285147/178767972-0c4f50b4-fe8b-410c-b0e2-991b81e40c9c.png)

# Problem Domain
![image](https://user-images.githubusercontent.com/39285147/178758777-4a467cce-9d42-485d-8068-096dc94d5ec1.png)

ì„œë¡œ ë‹¤ë¥¸ ë¶„ë¥˜ì„ ë“¤ì„ í•™ìŠµë°ì´í„°ì— ëŒ€í•˜ì—¬ í›Œë¥­í•˜ê²Œ ë¶„ë¥˜ë¥¼ í•´ëƒˆì§€ë§Œ, ì‹¤ìƒí™œì— ì ìš©í–ˆì„ ë•ŒëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ sampleë“¤ì´ í•´ë‹¹ ë¶„ë¥˜ì„ ë“¤ë¡œ í•˜ì—¬ê¸ˆ ë‹¤ë¥¸ ì„±ëŠ¥ì„ ëŒì–´ë‚¸ë‹¤.

# Support Vector Machine (SVM)
![image](https://user-images.githubusercontent.com/39285147/178759917-17b6d70d-5c49-45ff-a13b-e3a24e494d39.png)

Maximum margin hyperplane with support vectors
- Robust to outliers

> **Support vector*: an instance with the minimum margin, which will be the most sensible data points to affect the performance

## Margin
![image](https://user-images.githubusercontent.com/39285147/178760628-3ff33e56-e93a-4c7c-b309-e5ee7425c1c7.png)
![image](https://user-images.githubusercontent.com/39285147/178760767-9915ecd4-9605-4569-91cf-33e9bd7ea98e.png)

Twice the distance from the hyperplane to the nearest instance on either side

ğ‘¤ is orthogonal to the hyperplane

<details markdown="1">
<summary>Margin Distance(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178761055-d0eaaf0f-f7d9-48a1-8d1d-49959d0a59a2.png)

</details>

## Optimization
### 1. Hard margin SVM
![image](https://user-images.githubusercontent.com/39285147/178762447-30739e0a-ad58-46f7-a985-0b7ed11763bb.png)

Assumes linear separability

#### Constraints
![image](https://user-images.githubusercontent.com/39285147/178762996-62a0fcd6-ee7f-4960-aef9-7f083ca91f21.png)

### 2. Soft margin SVM
![image](https://user-images.githubusercontent.com/39285147/178763343-61e4ee2e-6ff6-4270-85d7-fb24c8e8b768.png)

Extends to non-separable cases

### 3. Nonlinear transform & kernel trick
![image](https://user-images.githubusercontent.com/39285147/178763530-2e4b28b7-b553-4bec-9a42-670a5b10f11d.png)

Linearly sepableí•˜ì§€ ì•Šì€ data sampleë“¤ì´ ìˆë‹¤ê³  í•  ë•Œ, ê·¸ ì°¨ìˆ˜ë¥¼ ë†’ì—¬ linearly sepableí•˜ê²Œ ë§Œë“œëŠ” ê³¼ì •.

#### Types of Kernels
![image](https://user-images.githubusercontent.com/39285147/178763662-4a870457-2ed7-4a06-a6ab-bae89e0a8d71.png)

<details markdown="1">
<summary>RBF(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178763860-50afdbe6-c92a-4316-8775-fac71ed76379.png)

</details>

# Artificial Neural Network (NN)
![image](https://user-images.githubusercontent.com/39285147/178764237-7dbf7df9-5de8-40be-8a8a-2b7d4d45a650.png)

ê²°ì´ ë‹¤ë¥¸ Classifier (Nonlinear Classification Model)

Deep Neural Network (DNN)ì˜ ê¸°ë³¸

### Activation Function
![image](https://user-images.githubusercontent.com/39285147/178764776-60aad5a1-f818-4d7a-8154-427e18a5a73a.png)

Scoreê°’ì„ Sigmoidì™€ ê°™ì€ í•¨ìˆ˜ì— ë„£ì–´ Nonlinear ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜

> *ReLU*: í•™ìŠµì„ ìˆ˜í–‰í•¨ì— ë”°ë¼ Gradient ê°’ì´ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” Sigmoidì™€ ë‹¬ë¦¬ ë¯¸ë¶„ê°’ì´ 1ë¡œ ê³ ì •ë˜ì–´ ê¾¸ì¤€íˆ í•™ìŠµì´ ê°€ëŠ¥í•œ ReLU

## Deep Neural Network (DNN)
![image](https://user-images.githubusercontent.com/39285147/178765564-08e48085-8e5f-48eb-9d98-6d16053c357d.png)

ìƒê¸° ANNì— ê³„ì¸µì„ ê¹Šê²Œ ìŒ“ìœ¼ë©´ DNNì´ ëœë‹¤. DNNëŠ” ê°ê°ì˜ ê³„ì¸µì— ë”°ë¼ì„œ í•™ìŠµì„ í•˜ê²Œ ë˜ëŠ” featureì˜ í˜•íƒœê°€ ë‹¬ë¼ì§„ë‹¤.

ì´ëŸ¬í•œ Nonlinear í•¨ìˆ˜ë“¤ì´ ê³„ì¸µì ìœ¼ë¡œ ìŒ“ì—¬ê°€ë©´ì„œ signal spadeì™€ ê°™ì€ ë³µì¡í•œ ì‹ í˜¸ë“¤ì˜ íŒ¨í„´ë“¤ì„ ì¡°ê¸ˆ ë” ì •í™•í•˜ê²Œ ë¶„ë¥˜ ê°€ëŠ¥í•˜ë‹¤.

## Multilayer Perceptron (MLP)
![image](https://user-images.githubusercontent.com/39285147/178765768-20a0994b-2e3c-4fde-9473-ee22fd4cfc6d.png)
![image](https://user-images.githubusercontent.com/39285147/178766731-66568a7a-2dea-43bf-b886-42ea03100d77.png)

Neural Networkë¥¼ ì—¬ëŸ¬ ê°œì˜ ì¸µìœ¼ë¡œ ìŒ“ì€ ê²ƒì´ Multilayer Perceptron Modelì´ë‹¤.

ê·¸ì „ê¹Œì§€ ì„ í˜• ëª¨ë¸ë¡œëŠ” í’€ ìˆ˜ ì—†ì—ˆë˜ XORì™€ ê°™ì€ ë¬¸ì œë“¤ì„ Multilayer Perceptron ê³„ì¸µë“¤ì„ ìŒ“ì•„ê°€ë©´ì„œ ë” ë³µì¡í•œ í˜•íƒœì˜ hyperplane í˜•ì„±ì´ ê°€ëŠ¥í•˜ë‹¤.

<details markdown="1">
<summary>XOR ë¬¸ì œ(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178766379-1c19213d-5193-4faf-877e-88fab16b84eb.png)
![image](https://user-images.githubusercontent.com/39285147/178766401-602caf14-f64b-4764-a367-e2f2ccf324f5.png)
![image](https://user-images.githubusercontent.com/39285147/178766422-c5c72322-bb45-4103-baf5-252c30e0ba13.png)
![image](https://user-images.githubusercontent.com/39285147/178766447-3911e480-e575-4baf-99a1-a5f52318ae1f.png)

</details>

### Example: MNIST data recognition
![image](https://user-images.githubusercontent.com/39285147/178766549-4cc2f940-d7ef-4743-9400-9e44e7e10ded.png)

ì´ë¯¸ì§€ patchê°€ ë“¤ì–´ì˜¬ ë•Œ, ê° ì´ë¯¸ì§€ ìˆ«ìë¥¼ 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ìë¡œ ì•Œë§ê²Œ ë¶„ë¥˜ë¥¼ í•˜ëŠ” ë¬¸ì œì´ë‹¤.

## í•œê³„ì 
### *Gradient Vanishing Problem*
![image](https://user-images.githubusercontent.com/39285147/178768444-6808fe2b-010b-43da-a5ec-123468d57e22.png)

The numerous multiplication of this result converges to near zero (= ì‹ ê²½ë§ ê¹Šì–´ì§ˆìˆ˜ë¡ í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠëŠ” í˜„ìƒ). 

## í•´ê²°ì 
- Backpropagation (BP)
- Pre training+ fine tuning
- *Convolutional neural networks (CNN)* for reducing redundant parameters.
- Rectified linear unit (constant gradient propagation)
- Dropout

## Convolutional neural networks (CNN)
![image](https://user-images.githubusercontent.com/39285147/178769285-615d4e95-245c-4910-9cd7-0c126faeaf8b.png)

State of the art classification model for high dimensional data (image, video, etc.)

# Quiz
![image](https://user-images.githubusercontent.com/39285147/178769502-e82d38c8-0cc0-48de-a978-7447bb85e4e2.png)
![image](https://user-images.githubusercontent.com/39285147/178769525-5fdc0788-d746-4d84-ac2e-51039ae3febf.png)

# References
- I. Goodfellow , et. al., â€œDeep Learningâ€, [online avaliable](http://www.deeplearningbook.org/)
- CS231n: Deep Learning for Computer Vision (http://cs231n.stanford.edu/)
