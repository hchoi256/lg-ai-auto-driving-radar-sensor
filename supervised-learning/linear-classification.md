****
### Terms
- Linear Classification
  - Zero-one loss
  - Hinge loss
  - Cross-entropy loss
- Score and Margin
- Multiclass classification

![image](https://user-images.githubusercontent.com/39285147/178240098-fcfabcdd-b93e-40a8-8271-f5665bb66e7e.png)

# Linear Classification
[*Linear Model*]

![image](https://user-images.githubusercontent.com/39285147/178746386-b49e5341-3b58-406e-9161-44d66d6e4da8.png)

Linear Classificationì€ decision boundaryë¥¼ ì°¾ì•„ë‚´ì–´ ì£¼ì–´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- Discrete output
- Use **labeled** dataset
  - Correct label is ready for a training set

## Advantage of linear classification
- Simple
- Interpretability (ì˜ˆì¸¡ ê²°ê³¼ì— ë”í•´ì„œ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µ)

<details markdown="1">
<summary>Interpretability(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178757022-bec942b7-6c75-415a-8c11-89d56c68f1c4.png)

</details>

## Goal: Find Target Function ğ‘“ (ğ‘“:ğ‘¿â†’ğ’€)
Hypothesis ğ‘”:ğ‘¿â†’ğ’€(ML model to approximate ğ‘“) : ğ‘”âˆˆğ‘¯

<details markdown="1">
<summary>ğ‘”âˆˆğ‘¯(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178739687-257c8740-fa15-46bc-bf10-052e3f505b87.png)- w 
![image](https://user-images.githubusercontent.com/39285147/178739877-de2f076b-afd9-463b-a548-6e93714a715f.png)

</details>

> Score ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ê²°ì •ê²½ê³„(h(x))ë¡œ ë¶€í„° ë©€ë¦¬ ë–¨ì–´ì§„ë‹¤.

<details markdown="1">
<summary>Score & Marginì´ë€?(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178741007-f2bb6a08-104f-4825-89ec-a5b961b3ba2d.png)

</details>

> Margin:  score * y(ì‹¤ì œê°’) (= ëª¨ë¸ì´ ì •ë‹µì„ ì˜ ë§ì¶”ê³  ìˆëŠ”ê°€)
>> ìŒì˜ ê°’: model prediction ì‹¤íŒ¨

>> ì–‘ì˜ ê°’: model prediction ì„±ê³µ

# Linear classification framework
![image](https://user-images.githubusercontent.com/39285147/178739145-94d4e15f-295e-4689-84b0-dadfaaae74aa.png)

## Zero-one loss
![image](https://user-images.githubusercontent.com/39285147/178741814-48468556-579e-4296-bfd9-79502b4f6d89.png)

ë‚´ë¶€ì˜ logicì„ íŒë³„í•˜ì—¬ ë§ìœ¼ë©´ 0 í‹€ë¦¬ë©´ 1ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜.

### í•œê³„ì™€ í•´ê²°ì 
Zero-one lossì— gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë ¤ë©´ partial derivative termì„ ì ìš©í•´ì•¼í•˜ë‚˜, ê¸°ìš¸ê¸°ê°€ ëª¨ë‘ 0ì´ ë˜ë²„ë¦°ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê³ ì, Hinge lossê°€ ì‚¬ìš©ëœë‹¤.

## Hinge loss
![image](https://user-images.githubusercontent.com/39285147/178741882-f726f06b-73e6-4b67-9483-78caed18459d.png)
 
ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ì˜ í•˜ê³  ìˆë‹¤ë©´(= Marginì´ 1ë³´ë‹¤ í¬ë‹¤ë©´), ìˆ˜ì‹ì— ì˜í•´ hinge lossëŠ” 0ì´ ëœë‹¤, vice versa.

## [Cross-entropy loss](https://github.com/EricChoii/ai-terms/blob/main/entropy.md)
> *Classification model í•™ìŠµì— ê°€ì¥ ë§ì´ ì‚¬ìš©ëœë‹¤*

![image](https://user-images.githubusercontent.com/39285147/178741907-3154da2e-6d7b-413a-94bf-7b766beedd57.png)

ì—”íŠ¸ë¡œí”¼ ê°’(H(p))ëŠ” ê³ ì •ê°’ì´ë¯€ë¡œ, ì‹¤ì§ˆì ìœ¼ë¡œ Cross entropyëŠ” KL divergence ê°’ì— ì˜í•´ ì¡°ì ˆëœë‹¤. ì—¬ê¸°ì„œ KL divergenceëŠ” pì™€ qì˜ ìœ ì‚¬ë„ì— ë”°ë¼ ê°’ì´ ì •í•´ì§„ë‹¤.
- pì™€ qê°€ ë‹¤ë¥´ë‹¤ë©´ cross entropy lossëŠ” ì¦ê°€í•œë‹¤.

### Score(ì‹¤ìˆ˜ê°’)ì„ í™•ë¥ ê°’ìœ¼ë¡œ Mappingí•˜ê¸°
ìš°ë¦¬ê°€ fittingì„ í•˜ê³ ì í•˜ëŠ” labelì€ 1 or 0ì¸ë°, ì‹¤ìˆ˜ê°’ì¸ scoreë¥¼ ì–´ë–»ê²Œ í™•ë¥  í•¨ìˆ˜ë¥¼ í†µí•´ ë§¤í•‘í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ **Sigmoid function*ê°€ ì‚¬ìš©ëœë‹¤.

<details markdown="1">
<summary>Sigmoid function(ì ‘ê¸°/í¼ì¹˜ê¸°)</summary>

![image](https://user-images.githubusercontent.com/39285147/178742116-8c589c94-5a4e-4603-bdb2-46d12e1f51d8.png)

(+)ê°’ìœ¼ë¡œ ì»¤ì§€ê²Œ ëœë‹¤ë©´ í™•ë¥  ê°’ 1ì— ê·¼ì‚¬í•˜ê²Œ ëœë‹¤, vice versa.

</details>

[*Logistic model*]

![image](https://user-images.githubusercontent.com/39285147/178741943-bc16c4b0-037f-4737-a878-4d16b6e5f614.png)

ì‹¤ìˆ˜ê°’ì¸ Scoreë¥¼ Sigmoid Functionì— ëŒ€ì…í•˜ì—¬ [0, 1] í™•ë¥  ë²”ìœ„ë¡œ ë¦¬í„´í•œë‹¤.

### Cross-entropy loss: Gradient Descent Method
![image](https://user-images.githubusercontent.com/39285147/178742070-8a89f201-3c40-43be-abc7-4b703a7c974e.png)

Lossê°€ ë°œìƒí•  ë•Œ, Gradient Descent ë°©ë²•ì„ í™œìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

# How to Train Linear Classifier - Gradient Descent
![image](https://user-images.githubusercontent.com/39285147/178742255-c2149c2c-75f0-489d-a93d-1a8572af4bdb.png)
- Gradient descent = Cross Entropy Lossì˜ ë¯¸ë¶„ê°’(= ![image](https://user-images.githubusercontent.com/39285147/178755058-37f42ba7-9b79-463c-9685-73f88b77f557.png))

Iterative optimization using gradient descent

1. Initialize weights at time step ğ’•= 0
2. Compute the gradients
3. Set the direction to move :
4. Update weights
5. Iterate to next step until converging

# Multiclass classification
![image](https://user-images.githubusercontent.com/39285147/178756035-e4294c2f-1fe7-4c35-b579-03b84b1dc377.png)
- Not all classification predictive models support multi class classification.
- split the multi class classification dataset into multiple binary classification datasets and fit a binary classification model on each.

### Example: Image Recognition
![image](https://user-images.githubusercontent.com/39285147/178746614-5194aebe-4686-4240-a22b-bb09402ef55f.png)

## Multiclass Classification One vs All
![image](https://user-images.githubusercontent.com/39285147/178742628-dfc16fb8-6ebb-4bdf-a048-f592cbd470c9.png)
![image](https://user-images.githubusercontent.com/39285147/178742769-142ac214-f9b8-49b1-83da-f7eb82d86d97.png)

One vs Allì€ binary classificationì—ì„œ multiclass classificationìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë¬¸ì œì´ë‹¤. ê° í•™ìŠµ ëª¨ë¸ì— ëŒ€í•œ Scoreê°’ì— Sigmoid Functionì„ ì·¨í•˜ì—¬ í™•ë¥ ê°’ìœ¼ë¡œ ë§¤í•‘í•˜ê³ , ì´ë¥¼ **One Hot Encoding*ìœ¼ë¡œ labelí™” ëœ ê°’ë“¤ê³¼ ë¹„êµí•˜ì—¬ errorë¥¼ ê³„ì‚°í•œë‹¤.

> One Hot Encoding: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í‘œ ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ í•˜ë©´ì„œ í•™ìŠµ

# Quiz
![image](https://user-images.githubusercontent.com/39285147/178743210-3c64eba8-bca0-4188-9dd1-50f35c8878aa.png)
![image](https://user-images.githubusercontent.com/39285147/178743234-891e5dfc-4840-4dad-85c5-a4a934f66407.png)
![image](https://user-images.githubusercontent.com/39285147/178743256-f44bdbab-b7a4-4b2f-bf6b-19c215583724.png)

# Reference
- Book: Pattern Recognition and Machine Learning (by Christopher M. Bishop)
- Book: Machine Learning: a Probabilistic Perspective (by Kevin P. Murphy)
- https://www.andrewng.org/courses/

